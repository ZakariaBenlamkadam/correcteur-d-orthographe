{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8604897,"sourceType":"datasetVersion","datasetId":5148840},{"sourceId":8604906,"sourceType":"datasetVersion","datasetId":5148845},{"sourceId":8604989,"sourceType":"datasetVersion","datasetId":5148899}],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Prepare Data","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\n\ndef split_to_sentences(data):\n    \"\"\"Z\n    Split data by linebreak \"\\n\" and clean the sentences.\n    \"\"\"\n    sentences = data.split(\"\\n\")\n    sentences = [s.strip() for s in sentences if s.strip()]\n    return sentences\n\ndef count_words(tokenized_sentences):\n    \"\"\"\n    Manually count the frequency of each word in the tokenized sentences.\n    \"\"\"\n    word_counts = {}\n    for sentence in tokenized_sentences:\n        for token in sentence:\n            if token not in word_counts:\n                word_counts[token] = 1\n            else:\n                word_counts[token] += 1\n    return word_counts\n\ndef get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n    \"\"\"\n    Filter words that appear at least 'count_threshold' times.\n    \"\"\"\n    closed_vocab = []\n    word_counts = count_words(tokenized_sentences)\n    for word, cnt in word_counts.items():\n        if cnt >= count_threshold:\n            closed_vocab.append(word)\n    return closed_vocab\n\ndef prepare_data(infile, ngram_size=2):\n    with open(infile, 'r', encoding='utf-8') as file:\n        text = file.read()\n    \n    sentences = split_to_sentences(text)\n    print(\"Number of sentences:\", len(sentences))  \n    \n    tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n    print(\"Sample tokenized sentences:\", tokenized_sentences[:3])  \n\n    vocabulary = get_words_with_nplus_frequency(tokenized_sentences, 3)\n    print(\"Vocabulary sample:\", list(vocabulary)[:10]) \n\n    processed_sentences = []\n    for sentence in tokenized_sentences:\n        processed_sentence = [token if token in vocabulary else '<UNK>' for token in sentence]\n        start_tokens = ['<s>'] * (ngram_size - 1)\n        end_tokens = ['<e>'] * (ngram_size - 1)\n        processed_sentence = start_tokens + processed_sentence + end_tokens\n        processed_sentences.append(processed_sentence)\n\n    print(\"Processed sentences sample:\", processed_sentences[:1])  \n    return processed_sentences\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-04T14:23:57.463506Z","iopub.execute_input":"2024-06-04T14:23:57.463863Z","iopub.status.idle":"2024-06-04T14:23:59.104789Z","shell.execute_reply.started":"2024-06-04T14:23:57.463833Z","shell.execute_reply":"2024-06-04T14:23:59.103795Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"processed_text = prepare_data('/kaggle/input/us-twitter/en_US.twitter.txt', ngram_size=2)\n ","metadata":{"execution":{"iopub.status.busy":"2024-06-04T14:26:00.714004Z","iopub.execute_input":"2024-06-04T14:26:00.715290Z","iopub.status.idle":"2024-06-04T14:26:36.718054Z","shell.execute_reply.started":"2024-06-04T14:26:00.715246Z","shell.execute_reply":"2024-06-04T14:26:36.717154Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Number of sentences: 47961\nSample tokenized sentences: [['how', 'are', 'you', '?', 'btw', 'thanks', 'for', 'the', 'rt', '.', 'you', 'gon', 'na', 'be', 'in', 'dc', 'anytime', 'soon', '?', 'love', 'to', 'see', 'you', '.', 'been', 'way', ',', 'way', 'too', 'long', '.'], ['when', 'you', 'meet', 'someone', 'special', '...', 'you', \"'ll\", 'know', '.', 'your', 'heart', 'will', 'beat', 'more', 'rapidly', 'and', 'you', \"'ll\", 'smile', 'for', 'no', 'reason', '.'], ['they', \"'ve\", 'decided', 'its', 'more', 'fun', 'if', 'i', 'do', \"n't\", '.']]\nVocabulary sample: ['how', 'are', 'you', '?', 'btw', 'thanks', 'for', 'the', 'rt', '.']\nProcessed sentences sample: [['<s>', 'how', 'are', 'you', '?', 'btw', 'thanks', 'for', 'the', 'rt', '.', 'you', 'gon', 'na', 'be', 'in', 'dc', 'anytime', 'soon', '?', 'love', 'to', 'see', 'you', '.', 'been', 'way', ',', 'way', 'too', 'long', '.', '<e>']]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(processed_text[:100]) ","metadata":{"execution":{"iopub.status.busy":"2024-06-04T14:26:44.197609Z","iopub.execute_input":"2024-06-04T14:26:44.197985Z","iopub.status.idle":"2024-06-04T14:26:44.203936Z","shell.execute_reply.started":"2024-06-04T14:26:44.197943Z","shell.execute_reply":"2024-06-04T14:26:44.202827Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[['<s>', 'how', 'are', 'you', '?', 'btw', 'thanks', 'for', 'the', 'rt', '.', 'you', 'gon', 'na', 'be', 'in', 'dc', 'anytime', 'soon', '?', 'love', 'to', 'see', 'you', '.', 'been', 'way', ',', 'way', 'too', 'long', '.', '<e>'], ['<s>', 'when', 'you', 'meet', 'someone', 'special', '...', 'you', \"'ll\", 'know', '.', 'your', 'heart', 'will', 'beat', 'more', 'rapidly', 'and', 'you', \"'ll\", 'smile', 'for', 'no', 'reason', '.', '<e>'], ['<s>', 'they', \"'ve\", 'decided', 'its', 'more', 'fun', 'if', 'i', 'do', \"n't\", '.', '<e>'], ['<s>', 'so', 'tired', 'd', ';', 'played', '<UNK>', 'tag', '&', 'ran', 'a', 'lot', 'd', ';', '<UNK>', 'going', 'to', 'sleep', 'like', 'in', '5', 'minutes', ';', ')', '<e>'], ['<s>', 'words', 'from', 'a', 'complete', 'stranger', '!', 'made', 'my', 'birthday', 'even', 'better', ':', ')', '<e>'], ['<s>', 'first', 'cubs', 'game', 'ever', '!', 'wrigley', 'field', 'is', 'gorgeous', '.', 'this', 'is', 'perfect', '.', 'go', 'cubs', 'go', '!', '<e>'], ['<s>', 'i', 'no', '!', 'i', 'get', 'another', 'day', 'off', 'from', 'skool', 'due', 'to', 'the', 'wonderful', 'snow', '(', ':', 'and', 'this', 'wakes', 'me', 'up', '...', 'damn', 'thing', '<e>'], ['<s>', 'i', \"'m\", 'coo', '...', 'jus', 'at', 'work', 'hella', 'tired', 'r', 'u', 'ever', 'in', 'cali', '<e>'], ['<s>', 'the', 'new', '<UNK>', 'commercial', '...', 'hehe', 'love', 'at', 'first', 'sight', '<e>'], ['<s>', 'we', 'need', 'to', 'reconnect', 'this', 'week', '<e>'], ['<s>', 'i', 'always', 'wonder', 'how', 'the', 'guys', 'on', 'the', 'auctions', 'shows', 'learned', 'to', 'talk', 'so', 'fast', '!', '?', 'all', 'i', 'hear', 'is', '<UNK>', '.', '<e>'], ['<s>', '<UNK>', 'what', 'a', 'catch', '<e>'], ['<s>', 'such', 'a', 'great', 'picture', '!', 'the', 'green', 'shirt', 'totally', 'brings', 'out', 'your', 'eyes', '!', '<e>'], ['<s>', 'desk', 'put', 'together', ',', 'room', 'all', 'set', 'up', '.', 'oh', 'boy', ',', 'oh', 'boy', '<e>'], ['<s>', 'i', \"'m\", 'doing', 'it', '!', '<UNK>', '<e>'], ['<s>', 'beauty', 'brainstorming', 'in', 'the', '<UNK>', 'office', 'with', 'and', 'sally', 'walker', '!', '<e>'], ['<s>', 'looking', 'for', 'a', 'new', 'band', 'to', 'blog', 'for', 'the', 'month', '.', 'anyone', 'interested', '?', '<e>'], ['<s>', 'packing', 'for', 'a', 'quick', 'move', 'down', 'the', 'street', '...', 'if', 'only', 'i', 'had', 'some', 'movers', '...', '<e>'], ['<s>', 'ford', 'focus', '<UNK>', '?', '<e>'], ['<s>', 'rt', ':', 'according', 'to', 'the', 'national', 'retail', 'federation', '$', '<UNK>', 'billion', 'was', 'spent', 'on', '#', 'mothersday', 'last', 'year', '!', '!', '<e>'], ['<s>', '“', ':', '``', 'the', 'tragedy', 'of', 'life', 'is', 'not', 'that', 'it', 'ends', 'so', 'soon', ',', 'but', 'that', 'we', 'wait', 'so', 'long', 'to', 'begin', 'it', '.', \"''\", '-', '<UNK>', '.', 'lewis', '”', '<e>'], ['<s>', 'more', 'skating', '!', 'come', 'by', 'the', 'check', 'out', 'a', 'movie', ',', 'eat', 'a', 'great', 'dinner', 'and', 'top', 'it', 'off', 'with', 'great', 'times', 'at', 'the', 'ice', 'rink', '.', '<e>'], ['<s>', 'watch', 'your', 'mailbox', '!', ':', ')', '<e>'], ['<s>', '<UNK>', 'the', 'day', '...', '<e>'], ['<s>', 'good', 'questions', '.', 'rt', ':', 'your', '#', 'brand', 'will', 'be', 'judged', 'based', 'on', 'its', '#', 'website', '.', 'is', 'your', 'website', 'a', 'good', 'brand', 'ambassador', '?', '...', '<e>'], ['<s>', 'do', \"n't\", 'care', 'what', 'others', 'think', 'of', 'you', ',', 'and', 'you', 'will', 'save', 'yourself', 'a', 'lot', 'of', 'mental', 'energy', 'that', 'instead', 'can', 'be', 'used', 'to', 'push', 'you', 'towards', 'success', '.', '<e>'], ['<s>', 'this', 'ron', 'artest', 'interview', '...', 'is', 'it', 'possible', 'to', 'die', 'from', 'laughter', '?', '<e>'], ['<s>', 'linda', '!', 'just', 'looked', 'at', 'my', '<UNK>', '&', 'i', 'have', 'to', 'hustle', 'back', 'to', 'chula', 'for', 'p.m.', 'meetings', ',', 'so', 'no', 'time', '4', 'lunch', '.', ':', '(', 'do', 'u', 'meet', 'every', 'fri', '?', '<e>'], ['<s>', 'bum', 'squad', 'lets', 'get', 'it', '!', 'rt', ':', 'shout', 'that', 'ninja', 'out', 'for', 'winning', '<e>'], ['<s>', 'i', 'love', 'you', ',', 'and', 'i', \"'m\", 'so', 'proud', 'of', 'you', '.', 'from', 'sitting', 'on', 'those', 'stairs', 'on', 'the', 'x', 'factor', ',', 'to', 'now', '.', 'you', 'boys', 'are', 'my', '<UNK>', ':', ')', 'xx', '<e>'], ['<s>', 'maybe', 'some', 'other', 'time', 'i', 'ca', \"n't\", 'slow', 'down', ',', 'right', 'across', 'that', 'state', 'line', 'right', 'about', 'now', '<e>'], ['<s>', '<UNK>', 'leak', '!', '<UNK>', '<e>'], ['<s>', 'i', 'love', 'reading', 'your', 'magazine', '(', ':', 'it', 'always', 'cheers', 'me', 'up', '<e>'], ['<s>', 'tables', 'are', 'all', 'sold', 'out', 'for', 'the', '<UNK>', '<UNK>', 'ball', '.', '<e>'], ['<s>', 'make', 'one', 'up', '!', 'it', 'might', 'help', 'you', 'feel', 'better', '.', 'alternative', ':', 'just', 'scream', 'nonsense', 'at', 'the', 'dumb', 'machine', '.', '<e>'], ['<s>', 'ya', 'ik', 'and', 'i', 'never', 'asked', 'him', 'to', 'follow', 'me', 'i', 'only', 'mentioned', 'him', 'once', 'in', 'one', 'of', 'my', '<UNK>', 'i', 'didnt', 'do', 'anything', 'else', '<e>'], ['<s>', 'i', 'will', '<', '3', '<e>'], ['<s>', 'great', 'talking', 'to', 'you', 'guys', 'tonight', '!', 'looking', 'forward', 'to', 'your', 'piece', 'next', 'week', 'jon', '.', '<e>'], ['<s>', 'small', 'market', 'baseball', '.', 'you', ',', 'know', '...', 'for', 'the', '99', '%', '.', '<e>'], ['<s>', 'sing', 'it', '!', '<e>'], ['<s>', 'it', 'comes', 'on', 'tonight', '!', '!', 'not', 'tomorrow', '!', '!', '<e>'], ['<s>', 'maybe', '?', 'just', 'seems', 'they', 'thought', 'up', 'that', 'idea', 'over', 'sunday', 'brunch', 'and', 'thought', 'it', 'was', 'swell', '.', '<e>'], ['<s>', 'thanks', 'for', 'the', '#', 'ff', '!', 'awfully', 'good', 'company', 'to', 'be', 'in', '.', '<e>'], ['<s>', '``', 'the', 'longer', 'we', 'live', 'the', 'more', 'we', 'find', 'we', 'are', 'like', 'other', 'persons', '.', \"''\", '~', '<UNK>', '<UNK>', 'holmes', '<e>'], ['<s>', 'made', 'my', 'list', 'for', 'top', '99', 'women', '.', '<e>'], ['<s>', 'nice', 'i', 'watched', 'the', 'whole', 'series', ',', 'loved', 'julia', 'and', 'her', 'mom', 'erica', 'was', 'such', 'a', 'badass', '<e>'], ['<s>', 'gop', 'line', 'on', 'obama', 'gay', 'marriage', 'stance', 'seems', 'to', 'be', 'that', 'he', '<UNK>', '.', 'really', 'want', 'to', 'use', 'that', 'with', 'romney', 'as', 'your', 'presidential', 'candidate', '?', '<e>'], ['<s>', 'i', 'know', ',', 'i', 'know', '.', 'then', 'you', 'kick', 'yourself', 'when', 'the', 'fight', 'goes', '<UNK>', '.', 'but', 'if', 'the', 'upset', 'does', 'happen', ',', 'wow', '.', 'nothing', 'like', 'it', '.', '<e>'], ['<s>', 'no', 'stress', 'balls', 'or', 'keg', 'and', 'different', 'crew', '--', 'but', 'these', 'guys', 'know', 'how', 'to', 'party', '<e>'], ['<s>', 'william', 'davis', 'attorney', 'says', 'client', 'will', '<UNK>', 'insanity', '.', 'he', 'faces', '<UNK>', 'murder', 'charge', 'in', 'woman', \"'s\", 'death', '<e>'], ['<s>', '#', '<UNK>', 'ga', ':', '``', 'i', 'always', 'considered', 'myself', 'a', 'liberal', 'until', 'i', 'saw', 'the', 'liberal', 'machine', 'in', 'action', '.', 'it', 'was', 'ugly', '.', \"''\", '<e>'], ['<s>', 'weird', 'thought', ':', 'we', 'got', 'to', 'witness', 'the', 'change', 'of', 'a', '<UNK>', ',', 'the', 'next', 'change', 'is', '40', 'generations', 'away', '...', 'imagine', 'what', 'the', 'differences', 'will', 'be', '!', '!', '<e>'], ['<s>', 'got', 'a', 'good', 'one', '?', '<e>'], ['<s>', 'love', 'chris', 'brown', '<e>'], ['<s>', 'wit', 'them', 'sexy', 'ass', 'lips', '<e>'], ['<s>', 'its', 'sounds', 'good', '<e>'], ['<s>', 'how', '(', 'un', '?', '?', ')', 'fortunate', 'that', 'those', 'days', 'when', 'every', 'status', 'update', 'was', 'a', 'song', 'lyrics', 'were', 'also', 'the', 'days', 'when', 'i', 'first', 'discovered', 'new', 'wave', '...', '<e>'], ['<s>', 'i', 'had', 'a', 'bomb', 'ass', 'day', 'chillin', \"'\", 'with', 'friends', '.', '<e>'], ['<s>', 'working', 'on', 'music', 'for', 'holiday', 'show', 'on', 'the', '23', 'in', 'santa', 'barbara', '!', '!', 'open', 'bar', '!', '!', 'me', ',', 'tim', 'from', 'palin', 'white', 't', \"'s\", 'and', 'jr', '<UNK>', 'from', '<UNK>', '!', '!', '<e>'], ['<s>', 'hey', 'nate', '!', 'thanks', 'for', 'dropping', 'by', 'yesterday', '.', 'how', 'was', 'your', 'meal', '?', '<e>'], ['<s>', '#', 'ff', ':', 'literary', 'lights', 'who', '<UNK>', 'my', 'week', '.', '<e>'], ['<s>', 'time', 'to', 'shape', 'up', '!', 'water', ',', '<UNK>', ',', 'weights', ',', 'and', '<UNK>', 'are', 'going', 'to', 'be', 'my', 'friends', '.', 'wedding', 'just', '4', 'months', 'away', '!', '<e>'], ['<s>', 'report', 'the', 'many', 'things', 'that', 'are', 'positive', 'about', 'the', 'university', 'of', 'arkansas', 'athletics', 'program', '.', '<e>'], ['<s>', '<UNK>', 'should', 'have', 'a', 'tool', 'to', 'charge', '$', '<UNK>', 'per', 'rt', 'request', '.', '$', 'to', 'support', '<UNK>', ',', 'fans', 'get', 'involved', '.', '<e>'], ['<s>', 'enjoy', '!', '!', 'stay', 'cool', '!', '!', '<e>'], ['<s>', '``', '...', 'yo', 'chick', 'she', 'so', '<UNK>', '...', \"''\", 'aye', '!', '!', '!', 'i', 'really', 'do', \"n't\", 'know', 'what', 'else', 'they', \"'re\", 'saying', 'tho', 'except', 'that', 'lol', '<e>'], ['<s>', 'talks', 'in', 'third', 'person', '.', '<e>'], ['<s>', '.', 'many', 'things', 'can', 'happen', '.', 'only', 'you', 'can', 'control', 'your', 'reaction', 'to', 'it', '.', 'practice', 'this', 'good', '<e>'], ['<s>', '``', 'you', 'will', 'allow', 'me', 'to', 'continue', 'to', 'do', 'what', 'i', 'love', '.', \"''\", 'big', 'show', \"'s\", 'wife', '?', '<e>'], ['<s>', 'haha', 'very', 'cute', '!', 'have', 'u', 'heard', 'from', 'julie', '?', '?', 'who', \"'s\", 'that', 'on', 'her', 'fb', 'i', \"'m\", 'tryin', 'not', 'to', 'laugh', '<e>'], ['<s>', 'and', 'somedays', 'youre', 'the', 'windshield', 'wiper', '<e>'], ['<s>', 'i', \"'m\", 'taking', 'adam', '!', ':', '-', ')', '<e>'], ['<s>', 'lets', 'do', 'this', '<e>'], ['<s>', 'not', 'much', ',', 'ben', '-', 'how', \"'s\", 'you', '?', '<e>'], ['<s>', 'sarah', 'trying', 'to', 'explain', 'to', 'my', 'teacher', 'yolo', ',', 'lmfao', '...', '.', '>', '>', '<e>'], ['<s>', '#', 'inspiring', 'rt', ':', 'great', 'minds', 'must', 'be', 'ready', 'not', 'only', 'to', 'take', 'opportunities', ',', 'but', 'to', 'make', 'them', '.', '--', '<UNK>', '<e>'], ['<s>', 'med', 'school', '...', 'wow', '!', 'i', 'could', 'never', 'do', 'that', '.', 'am', 'too', '<UNK>', 'to', 'be', 'around', 'blood', 'and', 'sick', 'people', 'for', 'long', '<UNK>', 'of', 'time', ';', ')', '<e>'], ['<s>', 'sleeping', '?', 'not', 'likely', '.', 'rt', ':', 'sleeping', 'to', '<e>'], ['<s>', 'time', 'for', 'lunch', '!', '<e>'], ['<s>', 'not', 'being', 'dependent', 'on', 'anyone', 'for', 'your', 'happiness', '>', '>', '>', '<e>'], ['<s>', 'this', '<UNK>', 'is', 'not', 'sitting', 'well', 'with', 'me', 'at', 'all', '!', '!', '#', '<UNK>', 'd', ':', '<e>'], ['<s>', 'do', 'u', 'know', 'whats', '<UNK>', '?', '?', '?', 'i', 'hate', 'cheesecake', '.', 'its', 'cake', 'with', 'cheese', 'in', 'front', 'of', 'it', '!', '<e>'], ['<s>', 'thanks', '.', 'got', 'it', 'at', 'walmart', '!', '<e>'], ['<s>', 'still', 'have', \"n't\", 'seen', '#', '<UNK>', '?', 'every', '<UNK>', 'check-in', 'wins', 'a', 'free', 'movie', 'ticket', '.', '<UNK>', 'check-ins', 'so', 'far', '!', '!', '<e>'], ['<s>', 'rt', 'congratulations', 'to', 'the', 'for', 'advancing', 'in', 'the', '#', '<UNK>', 'playoffs', '!', 'they', 'defeated', 'the', '#', '<UNK>', 'in', 'a', '4-3', 'series', '.', '<e>'], ['<s>', 'hey', 'hey', 'hey', ',', 'can', 'i', 'meet', '<UNK>', '?', '!', '<e>'], ['<s>', 'ghost', 'hunters', 'makes', 'me', 'cry', ':', '(', '<', '3', '<e>'], ['<s>', 'who', \"'s\", 'going', 'to', 'the', '<UNK>', 'show', 'tomorrow', '?', 'your', 'team', '<UNK>', 'will', 'be', 'in', 'the', 'house', '!', 'first', 'brew', 'after', 'the', 'show', 'is', 'on', 'me', '!', '<e>'], ['<s>', 'just', 'waking', 'up', 'but', '#', '<UNK>', 'last', 'night', 'though', '>', '>', '>', '>', '>', '>', '<e>'], ['<s>', 'at', 'six', 'flags', '.', '<e>'], ['<s>', 'thanks', 'tom', '.', '<e>'], ['<s>', 'they', \"'re\", 'not', '<UNK>', 'because', 'you', 'do', \"n't\", 'have', 'a', 'button', '!', '<e>'], ['<s>', '«', 'im', 'not', '<UNK>', 'it', 'just', 'sucks', 'that', 'i', 'cant', 'show', 'you', '<UNK>', '»', 'this', '!', '.', '<e>'], ['<s>', 'i', \"'m\", 'at', 'little', 'league', 'majors', 'try', 'outs', 'in', '<e>'], ['<s>', 'where', 'my', 'meatballs', 'at', '?', 'and', '<UNK>', ':', ')', '<e>'], ['<s>', 'doing', 'some', '#', '<UNK>', 'testing', 'and', '<UNK>', 'in', '#', 'wordpress', '.', 'what', 'the', '<UNK>', '<UNK>', 'think', 'of', '<UNK>', '?', '<e>'], ['<s>', 'lo', '<UNK>', '!', 'it', 'was', 'cool', 'experiencing', 'work', 'through', 'the', 'eyes', 'of', 'an', 'excited', 'teen', '<e>'], ['<s>', 'me', ':', 'the', 'only', 'b', 'word', 'u', 'should', 'call', 'a', 'girl', 'is', '<UNK>', '!', 'colin', ':', 'or', 'a', 'butterfly', '!', '<e>'], ['<s>', 'i', 'just', 'think', 'that', 'the', '<UNK>', 'is', '<UNK>', '.', 'it', 'needs', 'to', 'be', 'done', '<UNK>', '.', 'and', 'it', \"'s\", 'not', 'the', 'point', '.', '<e>'], ['<s>', 'if', 'you', 'have', 'firefox', 'or', 'google', 'chrome', 'type', 'in', '``', 'let', 'it', 'snow', \"''\", 'into', 'google', 'today', '.', '<e>']]\n","output_type":"stream"}]},{"cell_type":"code","source":"def count_n_grams(data, n):\n    \"\"\"\n    Count all n-grams in the data.\n\n    Args:\n        data: List of lists of words, each list representing tokenized sentences.\n        n: number of words in a sequence\n\n    Returns:\n        A dictionary that maps a tuple of n-words to its frequency\n    \"\"\"\n    n_grams = {}\n    for sentence in data:\n        for i in range(len(sentence) - n + 1):\n            n_gram = tuple(sentence[i:i + n])\n            if n_gram in n_grams:\n                n_grams[n_gram] += 1\n            else:\n                n_grams[n_gram] = 1\n    return n_grams\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T14:27:02.706242Z","iopub.execute_input":"2024-06-04T14:27:02.706686Z","iopub.status.idle":"2024-06-04T14:27:02.713189Z","shell.execute_reply.started":"2024-06-04T14:27:02.706656Z","shell.execute_reply":"2024-06-04T14:27:02.712222Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=5.0):\n    \"\"\"\n    Estimate the probabilities of a next word using the n-gram counts with k-smoothing.\n    \n    Args:\n        word: next word\n        previous_n_gram: A sequence of words of length n\n        n_gram_counts: Dictionary of counts of n-grams\n        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n        vocabulary_size: number of words in the vocabulary\n        k: positive constant, smoothing parameter\n    \n    Returns:\n        A probability\n    \"\"\"\n    previous_n_gram = tuple(previous_n_gram)\n    previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n    denominator = previous_n_gram_count + (k * vocabulary_size)\n    n_plus1_gram = previous_n_gram + (word,)\n    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram, 0)\n    numerator = n_plus1_gram_count + k\n    probability = numerator / denominator\n    return probability\n\ndef estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>', unknown_token=\"<unk>\", k=1.0):\n    \"\"\"\n    Estimate the probabilities of next words using the n-gram counts with k-smoothing.\n    \n    Args:\n        previous_n_gram: A sequence of words of length n\n        n_gram_counts: Dictionary of counts of n-grams\n        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n        vocabulary: List of words\n        k: positive constant, smoothing parameter\n    \n    Returns:\n        A dictionary mapping from next words to the probability.\n    \"\"\"\n    previous_n_gram = tuple(previous_n_gram)\n    vocabulary = set(vocabulary) | {end_token, unknown_token}\n    vocabulary_size = len(vocabulary)\n    probabilities = {}\n    for word in vocabulary:\n        probability = estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)\n        probabilities[word] = probability\n    return probabilities\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T14:27:06.752091Z","iopub.execute_input":"2024-06-04T14:27:06.752456Z","iopub.status.idle":"2024-06-04T14:27:06.764626Z","shell.execute_reply.started":"2024-06-04T14:27:06.752427Z","shell.execute_reply":"2024-06-04T14:27:06.763547Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def train(ngram_size, data, is_processed=False):\n    if not is_processed:\n        tokenized_data = prepare_data(data, ngram_size)\n    else:\n        tokenized_data = data\n\n    n_gram_counts = count_n_grams(tokenized_data, ngram_size)\n    n_plus1_gram_counts = count_n_grams(tokenized_data, ngram_size + 1)\n\n    vocabulary = set([token for sentence in tokenized_data for token in sentence])\n    vocabulary_size = len(vocabulary)\n\n    log_probabilities = {}\n    for n_gram in n_gram_counts:\n        for word in vocabulary:\n            n_plus1_gram = n_gram + (word,)\n            probability = estimate_probability(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0)\n            log_probabilities[n_plus1_gram] = math.log(probability)\n\n    return log_probabilities, vocabulary, n_gram_counts, n_plus1_gram_counts\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T14:27:12.276236Z","iopub.execute_input":"2024-06-04T14:27:12.277184Z","iopub.status.idle":"2024-06-04T14:27:12.284841Z","shell.execute_reply.started":"2024-06-04T14:27:12.277148Z","shell.execute_reply":"2024-06-04T14:27:12.283728Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def predict_ngram(sentence, model_probabilities, ngram_size=2, vocabulary=None):\n    \"\"\"\n    Predicts the probability of a sentence using a pre-trained n-gram model.\n    \n    Args:\n        sentence (str): The sentence to predict the probability for.\n        model_probabilities (dict): Precomputed n-gram probabilities from the trained model.\n        ngram_size (int): The size of the n-grams (2 for bigrams, 3 for trigrams).\n        vocabulary (set): Set of known vocabulary words from training.\n    \n    Returns:\n        float: The log probability of the sentence.\n    \"\"\"\n    # tokenizing and handling unknown words\n    tokenized_sentence = word_tokenize(sentence.lower())\n    processed_sentence = [token if token in vocabulary else '<UNK>' for token in tokenized_sentence]\n    start_tokens = ['<s>'] * (ngram_size - 1)\n    end_tokens = ['<e>'] * (ngram_size - 1)\n    processed_sentence = start_tokens + processed_sentence + end_tokens\n\n    # Calculating the probability of the sentence\n    log_probability = 0.0\n    for i in range(len(processed_sentence) - ngram_size + 1):\n        n_gram = tuple(processed_sentence[i:i + ngram_size])\n        if n_gram in model_probabilities:\n            log_probability += model_probabilities[n_gram]\n        else:\n            # Handle missing n-grams \n            log_probability += math.log(1.0 / len(vocabulary) ** ngram_size)\n\n    return log_probability\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T14:27:48.607696Z","iopub.execute_input":"2024-06-04T14:27:48.608362Z","iopub.status.idle":"2024-06-04T14:27:48.616513Z","shell.execute_reply.started":"2024-06-04T14:27:48.608329Z","shell.execute_reply":"2024-06-04T14:27:48.615457Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import math\nngram_size = 2  \ninfile = '/kaggle/input/trainandtest/ngramv1.train.txt'\nmodel_probabilities, vocabulary, n_gram_counts, n_plus1_gram_counts = train(ngram_size, infile)\n\n\nsentence = \"I AM SAM\"\npredicted_log_prob = predict_ngram(sentence, model_probabilities, ngram_size, vocabulary)\nprint(\"Log Probability of the sentence:\", predicted_log_prob)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T14:28:22.865796Z","iopub.execute_input":"2024-06-04T14:28:22.866173Z","iopub.status.idle":"2024-06-04T14:28:22.907182Z","shell.execute_reply.started":"2024-06-04T14:28:22.866144Z","shell.execute_reply":"2024-06-04T14:28:22.906274Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Number of sentences: 110\nSample tokenized sentences: [['i', 'am', 'sam', '.'], ['i', 'am', 'sam', '.'], ['sam', 'i', 'am', '.']]\nVocabulary sample: ['i', 'am', 'sam', '.', 'that', '!', 'do', 'not', 'like', 'would']\nProcessed sentences sample: [['<s>', 'i', 'am', 'sam', '.', '<e>']]\nLog Probability of the sentence: -31.296184043425168\n","output_type":"stream"}]},{"cell_type":"code","source":"import math\n\ndef test_perplexity(test_file, n_gram_counts, n_plus1_gram_counts, ngram_size=2, k=1.0):\n    \"\"\"\n    Calculate the perplexity of a language model on a given test corpus using pre-processed data.\n    \n    Args:\n        test_file (str): Path to the test corpus file.\n        n_gram_counts (dict): Dictionary of counts of n-grams.\n        n_plus1_gram_counts (dict): Dictionary of counts of (n+1)-grams.\n        ngram_size (int): The n-gram size (e.g., 2 for bigrams, 3 for trigrams).\n        k (float): Smoothing parameter.\n    \n    Returns:\n        float: The perplexity of the model on the test corpus.\n    \"\"\"\n    processed_sentences = prepare_data(test_file, ngram_size)\n    vocabulary = set(sum(processed_sentences, []))  \n    \n    total_log_prob = 0.0\n    total_tokens = 0\n    \n    for sentence in processed_sentences:\n        N = len(sentence) \n        for t in range(ngram_size, N):\n            n_gram = tuple(sentence[t - ngram_size:t])\n            word = sentence[t]\n            probability = estimate_probability(word, n_gram, n_gram_counts, n_plus1_gram_counts, len(vocabulary), k)\n            if probability > 0:\n                total_log_prob += math.log(probability)\n            else:\n                total_log_prob += math.log(1.0 / len(vocabulary))\n            total_tokens += 1\n\n    perplexity = math.exp(-total_log_prob / total_tokens)\n    return perplexity\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T14:29:08.820180Z","iopub.execute_input":"2024-06-04T14:29:08.821042Z","iopub.status.idle":"2024-06-04T14:29:08.830033Z","shell.execute_reply.started":"2024-06-04T14:29:08.821008Z","shell.execute_reply":"2024-06-04T14:29:08.829015Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"ngram_size = 2  \ntest_file = '/kaggle/input/trainandtest/ngramv1.test.txt'  \nperplexity = test_perplexity(test_file, n_gram_counts, n_plus1_gram_counts, ngram_size)\nprint(f\"Perplexity of the model on the test corpus: {perplexity}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T14:29:35.741412Z","iopub.execute_input":"2024-06-04T14:29:35.742299Z","iopub.status.idle":"2024-06-04T14:29:35.755503Z","shell.execute_reply.started":"2024-06-04T14:29:35.742264Z","shell.execute_reply":"2024-06-04T14:29:35.754549Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Number of sentences: 22\nSample tokenized sentences: [['say', '!'], ['i', 'like', 'green', 'eggs', 'and', 'ham', '!'], ['i', 'do', '!']]\nVocabulary sample: ['!', 'i', 'like', 'and', 'them', ',', 'eat', 'in', 'a', '.']\nProcessed sentences sample: [['<s>', '<UNK>', '!', '<e>']]\nPerplexity of the model on the test corpus: 15.132203778503982\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Handle big_data.txt","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import TweetTokenizer\n\ndef split_to_sentences(data):\n    \"\"\"\n    Split data by linebreak \"\\n\" and clean the sentences.\n    \"\"\"\n    sentences = data.split(\"\\n\")\n    sentences = [s.strip() for s in sentences if s.strip()]\n    return sentences\n\ndef count_words(tokenized_sentences):\n    \"\"\"\n    Manually count the frequency of each word in the tokenized sentences.\n    \"\"\"\n    word_counts = {}\n    for sentence in tokenized_sentences:\n        for token in sentence:\n            if token not in word_counts:\n                word_counts[token] = 1\n            else:\n                word_counts[token] += 1\n    return word_counts\n\ndef get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n    \"\"\"\n    Filter words that appear at least 'count_threshold' times.\n    \"\"\"\n    closed_vocab = []\n    word_counts = count_words(tokenized_sentences)\n    for word, cnt in word_counts.items():\n        if cnt >= count_threshold:\n            closed_vocab.append(word)\n    return closed_vocab\n\ndef prepare_data(infile, ngram_size=2):\n    \"\"\"\n    Prepare data for n-gram modeling by reading from a file, tokenizing, normalizing, \n    and adding start/end tokens.\n    \"\"\"\n    with open(infile, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n\n    sentences = split_to_sentences(text)\n    print(\"Number of sentences:\", len(sentences))  \n\n    tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in sentences]\n    print(\"Sample tokenized sentences:\", tokenized_sentences[:3])  \n\n    vocabulary = get_words_with_nplus_frequency(tokenized_sentences, 3)\n    print(\"Vocabulary sample:\", list(vocabulary)[:10])  \n\n    processed_sentences = []\n    for sentence in tokenized_sentences:\n        processed_sentence = [token if token in vocabulary else '<UNK>' for token in sentence]\n        start_tokens = ['<s>'] * (ngram_size - 1)\n        end_tokens = ['<e>'] * (ngram_size - 1)\n        full_sentence = start_tokens + processed_sentence + end_tokens  \n        processed_sentences.append(full_sentence)\n\n    print(\"Processed sentences sample:\", processed_sentences[:1])  \n    return processed_sentences, vocabulary\n\ninfile = '/kaggle/input/testtesttest/big.txt'  \nprocessed_data, vocab = prepare_data(infile, 2)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:10:45.112564Z","iopub.execute_input":"2024-06-04T18:10:45.113428Z","iopub.status.idle":"2024-06-04T18:11:59.207004Z","shell.execute_reply.started":"2024-06-04T18:10:45.113395Z","shell.execute_reply":"2024-06-04T18:11:59.206051Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Number of sentences: 103501\nSample tokenized sentences: [['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'adventures', 'of', 'sherlock', 'holmes'], ['by', 'sir', 'arthur', 'conan', 'doyle'], ['(', '#15', 'in', 'our', 'series', 'by', 'sir', 'arthur', 'conan', 'doyle', ')']]\nVocabulary sample: ['the', 'project', 'gutenberg', 'ebook', 'of', 'adventures', 'sherlock', 'holmes', 'by', 'sir']\nProcessed sentences sample: [['<s>', 'the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'adventures', 'of', 'sherlock', 'holmes', '<e>']]\n","output_type":"stream"}]},{"cell_type":"code","source":"def count_n_grams(data, n):\n    \"\"\"\n    Count all n-grams in the data.\n    Args:\n        data: List of lists of words, each list representing tokenized sentences.\n        n: number of words in an n-gram\n    Returns:\n        A dictionary that maps a tuple of n-words to its frequency\n    \"\"\"\n    n_grams = {}\n    for sentence in data:\n        for i in range(len(sentence) - n + 1):\n            n_gram = tuple(sentence[i:i + n])\n            if n_gram in n_grams:\n                n_grams[n_gram] += 1\n            else:\n                n_grams[n_gram] = 1\n    return n_grams\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:11:59.208988Z","iopub.execute_input":"2024-06-04T18:11:59.209312Z","iopub.status.idle":"2024-06-04T18:11:59.215755Z","shell.execute_reply.started":"2024-06-04T18:11:59.209285Z","shell.execute_reply":"2024-06-04T18:11:59.214776Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import math\n\ndef estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n    \"\"\"\n    Estimate the probabilities of a next word using the n-gram counts with k-smoothing.\n    Args:\n        word: next word\n        previous_n_gram: A sequence of words of length n\n        n_gram_counts: Dictionary of counts of n-grams\n        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n        vocabulary_size: number of words in the vocabulary\n        k: positive constant, smoothing parameter\n    Returns:\n        A probability\n    \"\"\"\n    previous_n_gram = tuple(previous_n_gram)\n    previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n    denominator = previous_n_gram_count + (k * vocabulary_size)\n    n_plus1_gram = previous_n_gram + (word,)\n    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram, 0)\n    numerator = n_plus1_gram_count + k\n    probability = numerator / denominator\n    return probability\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:11:59.216904Z","iopub.execute_input":"2024-06-04T18:11:59.217184Z","iopub.status.idle":"2024-06-04T18:11:59.226574Z","shell.execute_reply.started":"2024-06-04T18:11:59.217162Z","shell.execute_reply":"2024-06-04T18:11:59.225580Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import math\n\ndef train(ngram_size, data, is_processed=False):\n    \"\"\"\n    Train an n-gram model and calculate log probabilities with add-k smoothing.\n    Args:\n        ngram_size (int): The n-gram size (2 for bigram, 3 for trigram).\n        data (str or list): Path to the training corpus file or preprocessed data.\n        is_processed (bool): Flag to indicate if the data is already preprocessed.\n    Returns:\n        tuple: A dictionary with n-gram tuples as keys and log probabilities as values, and the vocabulary.\n    \"\"\"\n    if not is_processed:\n        tokenized_data, vocabulary = prepare_data(data, ngram_size)  # assuming prepare_data returns vocabulary\n    else:\n        tokenized_data, vocabulary = data\n\n    n_gram_counts = count_n_grams([tuple(sentence) for sentence in tokenized_data], ngram_size)  # Convert lists to tuples\n    n_plus1_gram_counts = count_n_grams([tuple(sentence) for sentence in tokenized_data], ngram_size + 1)  # Convert lists to tuples\n    vocabulary_size = len(vocabulary)\n\n    log_probabilities = {}\n    for n_gram in n_gram_counts:\n        for word in vocabulary:\n            probability = estimate_probability(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0)\n            key = n_gram + (word,)\n            log_probabilities[key] = math.log(probability)\n\n    return log_probabilities, vocabulary","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:11:59.228402Z","iopub.execute_input":"2024-06-04T18:11:59.228733Z","iopub.status.idle":"2024-06-04T18:11:59.239127Z","shell.execute_reply.started":"2024-06-04T18:11:59.228709Z","shell.execute_reply":"2024-06-04T18:11:59.238278Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef generate_text(model_probabilities, ngram_size=2, start_token='<s>', end_token='<e>'):\n    \"\"\"\n    Generate text using an n-gram model.\n    Args:\n        model_probabilities (dict): A dictionary where keys are tuples representing n-grams\n                                    and values are probabilities of these n-grams.\n        ngram_size (int): The size of the n-grams used in the model.\n        start_token (str): The token that denotes the start of a sentence.\n        end_token (str): The token that denotes the end of a sentence.\n    Returns:\n        str: Generated text.\n    \"\"\"\n    current_tokens = [start_token] * (ngram_size - 1)\n    sentence = []\n\n    while True:\n        possible_tokens = {}\n        for n_gram, prob in model_probabilities.items():\n            if tuple(current_tokens) == n_gram[:ngram_size - 1]:\n                possible_tokens[n_gram[-1]] = prob\n\n        if not possible_tokens:\n            break\n\n        next_token = np.random.choice(list(possible_tokens.keys()), p=[prob/sum(possible_tokens.values()) for prob in possible_tokens.values()])\n\n        if next_token == end_token:\n            break\n\n        sentence.append(next_token)\n        current_tokens = current_tokens[1:] + [next_token]\n\n    return ' '.join(sentence)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:11:59.240283Z","iopub.execute_input":"2024-06-04T18:11:59.240564Z","iopub.status.idle":"2024-06-04T18:11:59.251312Z","shell.execute_reply.started":"2024-06-04T18:11:59.240541Z","shell.execute_reply":"2024-06-04T18:11:59.250415Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\ninfile = '/kaggle/input/trainandtest/ngramv1.train.txt'  # Make sure this path is correct and the file exists\n\nngram_size = 2  \n\nprint(\"Training the model...\")\nmodel_probabilities, vocabulary = train(ngram_size, infile, is_processed=False)\nprint(\"Training completed.\")\n\nprint(\"Generating text...\")\ngenerated_text = generate_text(model_probabilities, ngram_size=2)\nprint(\"Generated Text:\", generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:11:59.253232Z","iopub.execute_input":"2024-06-04T18:11:59.253553Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Training the model...\nNumber of sentences: 110\nSample tokenized sentences: [['i', 'am', 'sam', '.'], ['i', 'am', 'sam', '.'], ['sam', 'i', 'am', '.']]\nVocabulary sample: ['i', 'am', 'sam', '.', 'that', '!', 'do', 'not', 'like', 'would']\nProcessed sentences sample: [['<s>', 'i', 'am', 'sam', '.', '<e>']]\nTraining completed.\nGenerating text...\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}