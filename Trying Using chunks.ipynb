{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywJFO-JjiIlJ"
      },
      "source": [
        "\n",
        "## BIG.DATA.TXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nmSw4bI9fchk"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from collections import Counter\n",
        "\n",
        "def split_to_sentences(data):\n",
        "    \"\"\"Split data by linebreak \"\\n\" and clean the sentences.\"\"\"\n",
        "    sentences = data.split(\"\\n\")\n",
        "    return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "def prepare_data_two_pass(infile, ngram_size=2, freq_threshold=3):\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "\n",
        "    # First pass to build vocabulary\n",
        "    word_counts = Counter()\n",
        "    with open(infile, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                tokens = tokenizer.tokenize(line)\n",
        "                word_counts.update(tokens)\n",
        "\n",
        "    # Filter words based on the frequency threshold\n",
        "    vocabulary = {word for word, count in word_counts.items() if count >= freq_threshold}\n",
        "\n",
        "    # Second pass to process sentences and apply vocabulary filter\n",
        "    processed_sentences = []\n",
        "    with open(infile, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                tokens = tokenizer.tokenize(line)\n",
        "                processed_sentence = [token if token in vocabulary else '<UNK>' for token in tokens]\n",
        "                # Add start and end tokens\n",
        "                processed_sentence = ['<s>'] * (ngram_size - 1) + processed_sentence + ['<e>'] * (ngram_size - 1)\n",
        "                processed_sentences.append(processed_sentence)\n",
        "\n",
        "    return processed_sentences, vocabulary\n",
        "\n",
        "# Example usage\n",
        "infile = '/content/sample_data/en_US.twitter.txt'\n",
        "processed_data, vocab = prepare_data_two_pass(infile, 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "22GMio-WixjI"
      },
      "outputs": [],
      "source": [
        "def count_n_grams(data, n):\n",
        "    \"\"\"\n",
        "    Count all n-grams in the data.\n",
        "    Args:\n",
        "        data: List of lists of words, each list representing tokenized sentences.\n",
        "        n: number of words in an n-gram\n",
        "    Returns:\n",
        "        A dictionary that maps a tuple of n-words to its frequency\n",
        "    \"\"\"\n",
        "    n_grams = {}\n",
        "    for sentence in data:\n",
        "        # Ensure the entire list from i to i+n is converted into a tuple\n",
        "        sentence_n_grams = [tuple(sentence[i:i + n]) for i in range(len(sentence) - n + 1)]\n",
        "        for n_gram in sentence_n_grams:\n",
        "            if n_gram in n_grams:\n",
        "                n_grams[n_gram] += 1\n",
        "            else:\n",
        "                n_grams[n_gram] = 1\n",
        "    return n_grams\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "etoBHGTcixl8"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "    \"\"\"\n",
        "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing.\n",
        "    Args:\n",
        "        word: next word\n",
        "        previous_n_gram: A sequence of words of length n\n",
        "        n_gram_counts: Dictionary of counts of n-grams\n",
        "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "        vocabulary_size: number of words in the vocabulary\n",
        "        k: positive constant, smoothing parameter\n",
        "    Returns:\n",
        "        A probability\n",
        "    \"\"\"\n",
        "    previous_n_gram = tuple(previous_n_gram)\n",
        "    previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n",
        "    denominator = previous_n_gram_count + (k * vocabulary_size)\n",
        "    n_plus1_gram = previous_n_gram + (word,)\n",
        "    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram, 0)\n",
        "    numerator = n_plus1_gram_count + k\n",
        "    probability = numerator / denominator\n",
        "    return probability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-Ku3DC0BixrB"
      },
      "outputs": [],
      "source": [
        "def train(ngram_size, infile, is_processed=False):\n",
        "    \"\"\"\n",
        "    Train an n-gram model and calculate log probabilities with add-k smoothing.\n",
        "\n",
        "    Args:\n",
        "        ngram_size (int): The n-gram size (e.g., 2 for bigram, 3 for trigram).\n",
        "        infile (str): Path to the training corpus file or preprocessed data.\n",
        "        is_processed (bool): Flag to indicate if the data is already preprocessed.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A dictionary with n-gram tuples as keys and log probabilities as values, and the vocabulary.\n",
        "    \"\"\"\n",
        "    if not is_processed:\n",
        "        processed_data, vocabulary = prepare_data_two_pass(infile, ngram_size)\n",
        "    else:\n",
        "        processed_data, vocabulary = infile  # assuming infile is a tuple (data, vocab) when is_processed=True\n",
        "\n",
        "    n_gram_counts = count_n_grams(processed_data, ngram_size)\n",
        "    n_plus1_gram_counts = count_n_grams(processed_data, ngram_size + 1)\n",
        "\n",
        "    vocabulary_size = len(vocabulary)\n",
        "    log_probabilities = {}\n",
        "    for n_gram in n_gram_counts:\n",
        "        for word in vocabulary:\n",
        "            probability = estimate_probability(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size)\n",
        "            log_probabilities[n_gram + (word,)] = math.log(probability)\n",
        "\n",
        "    return log_probabilities, vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KruFz2HUixtS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_text(model_probabilities, ngram_size=2, start_token='<s>', end_token='<e>'):\n",
        "    \"\"\"\n",
        "    Generate text using an n-gram model.\n",
        "    Args:\n",
        "        model_probabilities (dict): A dictionary where keys are tuples representing n-grams\n",
        "                                    and values are probabilities of these n-grams.\n",
        "        ngram_size (int): The size of the n-grams used in the model.\n",
        "        start_token (str): The token that denotes the start of a sentence.\n",
        "        end_token (str): The token that denotes the end of a sentence.\n",
        "    Returns:\n",
        "        str: Generated text.\n",
        "    \"\"\"\n",
        "    current_tokens = [start_token] * (ngram_size - 1)\n",
        "    sentence = []\n",
        "\n",
        "    while True:\n",
        "        possible_tokens = {}\n",
        "        for n_gram, prob in model_probabilities.items():\n",
        "            if tuple(current_tokens) == n_gram[:ngram_size - 1]:\n",
        "                possible_tokens[n_gram[-1]] = prob\n",
        "\n",
        "        if not possible_tokens:\n",
        "            break\n",
        "\n",
        "        next_token = np.random.choice(list(possible_tokens.keys()), p=[prob/sum(possible_tokens.values()) for prob in possible_tokens.values()])\n",
        "\n",
        "        if next_token == end_token:\n",
        "            break\n",
        "\n",
        "        sentence.append(next_token)\n",
        "        current_tokens = current_tokens[1:] + [next_token]\n",
        "\n",
        "    return ' '.join(sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "ZHUVaUTZixwB",
        "outputId": "3c0597aa-adde-4fef-8d43-e3685fccf52a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model...\n",
            "Training completed.\n",
            "Generating text...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b94ee5bcdc92>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# generate text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating text...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_probabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generated Text:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-3176db529eb5>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model_probabilities, ngram_size, start_token, end_token)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mpossible_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn_gram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_probabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_tokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn_gram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mngram_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0mpossible_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_gram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "infile = '/content/sample_data/ngramv1.train.txt'\n",
        "ngram_size = 2\n",
        "\n",
        "# Train the model\n",
        "print(\"Training the model...\")\n",
        "model_probabilities, vocabulary = train(ngram_size=ngram_size, infile=infile, is_processed=False)\n",
        "print(\"Training completed.\")\n",
        "\n",
        "# generate text\n",
        "print(\"Generating text...\")\n",
        "generated_text = generate_text(model_probabilities, ngram_size=2)\n",
        "print(\"Generated Text:\", generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlUSepLQjzvW"
      },
      "source": [
        "\n",
        "\n",
        "##Trying to train using chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KKOxLSgHseBP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "28uEiGTDrstG"
      },
      "outputs": [],
      "source": [
        "def read_in_chunks(file_path, chunk_size=1024 * 1024):\n",
        "    \"\"\"Lazy function to read a file piece by piece. Default chunk size: 1MB.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        while True:\n",
        "            chunk = file.readlines(chunk_size)\n",
        "            if not chunk:\n",
        "                break\n",
        "            yield chunk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PY_xb2wTrswC"
      },
      "outputs": [],
      "source": [
        "def build_vocabulary(file_path, tokenizer, chunk_size=1024 * 1024):\n",
        "    word_counts = Counter()\n",
        "    for chunk in read_in_chunks(file_path, chunk_size):\n",
        "        for line in chunk:\n",
        "            tokens = tokenizer.tokenize(line.strip())\n",
        "            word_counts.update(tokens)\n",
        "    # Only retain words that appear at least 3 times\n",
        "    return {word for word, count in word_counts.items() if count >= 3}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kjX1K4t8rsys"
      },
      "outputs": [],
      "source": [
        "def process_sentences(file_path, tokenizer, vocabulary, ngram_size, chunk_size=1024 * 1024):\n",
        "    processed_sentences = []\n",
        "    for chunk in read_in_chunks(file_path, chunk_size):\n",
        "        for line in chunk:\n",
        "            tokens = tokenizer.tokenize(line.strip())\n",
        "            processed_sentence = [token if token in vocabulary else '<UNK>' for token in tokens]\n",
        "            # Add start and end tokens\n",
        "            processed_sentence = ['<s>'] * (ngram_size - 1) + processed_sentence + ['<e>'] * (ngram_size - 1)\n",
        "            processed_sentences.append(processed_sentence)\n",
        "    return processed_sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DwAl2y7aszVG"
      },
      "outputs": [],
      "source": [
        "def count_n_grams(data, n):\n",
        "    \"\"\"\n",
        "    Count all n-grams in the data.\n",
        "    Args:\n",
        "        data: List of lists of words, each list representing tokenized sentences.\n",
        "        n: Number of words in an n-gram.\n",
        "    Returns:\n",
        "        A dictionary that maps a tuple of n-words to its frequency.\n",
        "    \"\"\"\n",
        "    n_grams = {}\n",
        "    for sentence in data:\n",
        "        # Create n-grams from the sentence\n",
        "        sentence_n_grams = [tuple(sentence[i:i + n]) for i in range(len(sentence) - n + 1)]\n",
        "        for n_gram in sentence_n_grams:\n",
        "            if n_gram in n_grams:\n",
        "                n_grams[n_gram] += 1\n",
        "            else:\n",
        "                n_grams[n_gram] = 1\n",
        "    return n_grams\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ReaO15vMszX9"
      },
      "outputs": [],
      "source": [
        "def estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "    \"\"\"\n",
        "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing.\n",
        "    Args:\n",
        "        word: Next word to estimate probability for.\n",
        "        previous_n_gram: A tuple representing the previous words.\n",
        "        n_gram_counts: Dictionary of n-gram counts.\n",
        "        n_plus1_gram_counts: Dictionary of (n+1)-gram counts.\n",
        "        vocabulary_size: Total number of words in the vocabulary.\n",
        "        k: Smoothing parameter.\n",
        "    Returns:\n",
        "        The estimated probability.\n",
        "    \"\"\"\n",
        "    previous_n_gram = tuple(previous_n_gram)\n",
        "    previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n",
        "    denominator = previous_n_gram_count + k * vocabulary_size\n",
        "    n_plus1_gram = previous_n_gram + (word,)\n",
        "    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram, 0)\n",
        "    numerator = n_plus1_gram_count + k\n",
        "    probability = numerator / denominator\n",
        "    return probability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "j4gMzbL7rs08"
      },
      "outputs": [],
      "source": [
        "def train(ngram_size, infile, chunk_size=1024 * 1024):\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "    vocabulary = build_vocabulary(infile, tokenizer, chunk_size)\n",
        "    processed_data = process_sentences(infile, tokenizer, vocabulary, ngram_size, chunk_size)\n",
        "\n",
        "    n_gram_counts = count_n_grams(processed_data, ngram_size)\n",
        "    n_plus1_gram_counts = count_n_grams(processed_data, ngram_size + 1)\n",
        "\n",
        "    vocabulary_size = len(vocabulary)\n",
        "    log_probabilities = {}\n",
        "    for n_gram in n_gram_counts:\n",
        "        for word in vocabulary:\n",
        "            probability = estimate_probability(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size)\n",
        "            log_probabilities[n_gram + (word,)] = math.log(probability)\n",
        "\n",
        "    return log_probabilities, vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fmRxXiW0rs3z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_text(model_probabilities, ngram_size=2, start_token='<s>', end_token='<e>', max_length=100):\n",
        "    \"\"\"\n",
        "    Generate text using an n-gram model.\n",
        "    Args:\n",
        "        model_probabilities (dict): A dictionary where keys are tuples representing n-grams\n",
        "                                    and values are probabilities of these n-grams.\n",
        "        ngram_size (int): The size of the n-grams used in the model.\n",
        "        start_token (str): The token that denotes the start of a sentence.\n",
        "        end_token (str): The token that denotes the end of a sentence.\n",
        "        max_length (int): Maximum length of the generated sentence to prevent infinite loops.\n",
        "    Returns:\n",
        "        str: Generated text.\n",
        "    \"\"\"\n",
        "    current_tokens = [start_token] * (ngram_size - 1)\n",
        "    sentence = []\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        possible_tokens = {}\n",
        "        for n_gram, log_prob in model_probabilities.items():\n",
        "            if tuple(current_tokens) == n_gram[:ngram_size - 1]:\n",
        "                possible_tokens[n_gram[-1]] = np.exp(log_prob)  # Convert log probabilities back to probabilities\n",
        "\n",
        "        if not possible_tokens:\n",
        "            break\n",
        "\n",
        "        next_tokens = list(possible_tokens.keys())\n",
        "        probabilities = [possible_tokens[next] for next in next_tokens]\n",
        "        probabilities /= np.sum(probabilities)  # Normalize to form a probability distribution\n",
        "\n",
        "        next_token = np.random.choice(next_tokens, p=probabilities)\n",
        "\n",
        "        if next_token == end_token:\n",
        "            break\n",
        "\n",
        "        sentence.append(next_token)\n",
        "        current_tokens = current_tokens[1:] + [next_token]\n",
        "\n",
        "    return ' '.join(sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTQtVVJZrs6i",
        "outputId": "712a3df8-1491-4a8a-cea2-e81b556eed2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model...\n",
            "Training completed.\n",
            "Generating text...\n",
            "Generated Text: eggs ham would say train mouse say do goat let the try let could will goat train mouse goat eggs dark dark green see box say do them anywhere let on a that ham rain the sam sam could , on eat do let be be and you fox sam ! , goat eat here them like mouse eat am the anywhere anywhere mouse me rain rain ham like sam green ? the car i car i i fox house do tree fox not say anywhere them mouse box goat with fox here ? see there dark dark green you\n"
          ]
        }
      ],
      "source": [
        "infile = '/content/sample_data/ngramv1.train.txt'\n",
        "ngram_size = 2\n",
        "\n",
        "# Train the model\n",
        "print(\"Training the model...\")\n",
        "model_probabilities, vocabulary = train(ngram_size=ngram_size, infile=infile)\n",
        "print(\"Training completed.\")\n",
        "\n",
        "# Generate text\n",
        "print(\"Generating text...\")\n",
        "generated_text = generate_text(model_probabilities, ngram_size=ngram_size)\n",
        "print(\"Generated Text:\", generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJ_02pzltnPg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}